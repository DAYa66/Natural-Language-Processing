{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0fee7eb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Задание 1. Написать теггер на данных с русским языком\n",
    "\n",
    "    -   проверить UnigramTagger, BigramTagger, TrigramTagger и их комбмнации\n",
    "    -   написать свой теггер как на занятии, попробовать разные векторайзеры, добавить знание не только букв но и слов\n",
    "    -   сравнить все реализованные методы сделать выводы\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c374f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download() \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd475a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af7c3cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.corpus import brown\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger, TrigramTagger\n",
    "from nltk.tag import RegexpTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86519cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyconll\n",
      "  Downloading pyconll-3.1.0-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: pyconll\n",
      "Successfully installed pyconll-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyconll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ded23e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyconll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51810d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-29 21:43:04--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
      "Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 40736581 (39M) [text/plain]\n",
      "Сохранение в: ‘./data/ru_syntagrus-ud-train.conllu’\n",
      "\n",
      "./data/ru_syntagrus 100%[===================>]  38,85M  9,02MB/s    за 4,7s    \n",
      "\n",
      "2023-01-29 21:43:09 (8,21 MB/s) - ‘./data/ru_syntagrus-ud-train.conllu’ сохранён [40736581/40736581]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O ./data/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b05ad4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-29 21:44:00--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu\n",
      "Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 14704579 (14M) [text/plain]\n",
      "Сохранение в: ‘./data/ru_syntagrus-ud-dev.conllu’\n",
      "\n",
      "./data/ru_syntagrus 100%[===================>]  14,02M  7,05MB/s    за 2,0s    \n",
      "\n",
      "2023-01-29 21:44:03 (7,05 MB/s) - ‘./data/ru_syntagrus-ud-dev.conllu’ сохранён [14704579/14704579]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O ./data/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e867fbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\t     NER-2.ipynb\t   NER_rus.ipynb\r\n",
      "image\t     ner_nn.ipynb\t   Pos_teggers.ipynb\r\n",
      "NER-1.ipynb  NER_restaurant.ipynb  ru_syntagrus-ud-train-a.conllu\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8f4dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = pyconll.load_from_file('data/ru_syntagrus-ud-train.conllu')\n",
    "full_test = pyconll.load_from_file('data/ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c49b125a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Однако ADV\n",
      "при ADP\n",
      "включении NOUN\n",
      "метода NOUN\n",
      "генерации NOUN\n",
      "случайных ADJ\n",
      "чисел NOUN\n",
      "в ADP\n",
      "список NOUN\n",
      "\" PUNCT\n",
      "исходных ADJ\n",
      "данных NOUN\n",
      "\" PUNCT\n",
      ", PUNCT\n",
      "вероятностный ADJ\n",
      "алгоритм NOUN\n",
      "становится VERB\n",
      "подвидом NOUN\n",
      "обычного ADJ\n",
      ". PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in full_test[23:24]:\n",
    "    for token in sent:\n",
    "        print(token.form, token.upos)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e64ac83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in full_test[23:24]:\n",
    "    for token in sent:\n",
    "        print(type(token.form))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0c9635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# удалил из датасета неинформативную пунктуацию\n",
    "fdata_train = []\n",
    "for sent in full_train[:]:\n",
    "    fdata_train.append([(token.form, token.upos) for token in sent if token.upos not in ['PUNCT']])\n",
    "    \n",
    "fdata_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_test.append([(token.form, token.upos) for token in sent if token.upos not in ['PUNCT']])\n",
    "    \n",
    "fdata_sent_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_sent_test.append([token.form for token in sent if token.upos not in ['PUNCT']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf5c5f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наибольшая длина предложения 194\n",
      "Наибольшая длина токена 31\n"
     ]
    }
   ],
   "source": [
    "MAX_SENT_LEN = max(len(sent) for sent in full_train)\n",
    "MAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent)\n",
    "print('Наибольшая длина предложения', MAX_SENT_LEN)\n",
    "print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "144008ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ORIG_TOKEN = sorted((len(token.form), token.form) for sent in full_train for token in sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62189138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, '!'), (1, '!'), (1, '!'), (1, '!'), (1, '!')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_ORIG_TOKEN[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef3568ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(31, 'профессорско-преподавательского'),\n",
       " (30, 'потребительско-патерналистский'),\n",
       " (28, 'фаталистически-расслабленное'),\n",
       " (27, 'информационно-навигационную'),\n",
       " (26, 'художественно-эстетических'),\n",
       " (26, 'спектрально-энергетический'),\n",
       " (26, 'социально-реабилитационных'),\n",
       " (26, 'неинституционализированная'),\n",
       " (26, 'национал-социалистического'),\n",
       " (26, 'культурно-просветительских'),\n",
       " (26, 'Петропавловска-Камчатского'),\n",
       " (25, 'социально-психологических'),\n",
       " (25, 'партийно-государственнной'),\n",
       " (25, 'общественно-экономическую'),\n",
       " (25, 'общественно-политического'),\n",
       " (25, 'национал-социалистической'),\n",
       " (25, 'взаимно-перпендикулярными'),\n",
       " (25, 'Национал-социалистическая'),\n",
       " (24, 'финансово-олигархической'),\n",
       " (24, 'ультранационалистических')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(MAX_ORIG_TOKEN, reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35d50f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анкета\n",
      "Начальник областного управления связи Семен Еремеевич был человек простой приходил на работу всегда вовремя здоровался с секретаршей за руку и иногда даже писал в стенгазету заметки под псевдонимом Муха\n",
      "В приемной его с утра ожидали посетители кое-кто с важными делами а кое-кто и с такими которые легко можно было решить в нижестоящих инстанциях не затрудняя Семена Еремеевича\n",
      "Однако стиль работы Семена Еремеевича заключался в том чтобы принимать всех желающих и лично вникать в дело\n",
      "Приемная была обставлена просто но по-деловому\n",
      "У двери стоял стол секретарши на столе пишущая машинка с широкой кареткой\n",
      "В углу висел репродуктор и играло радио для развлечения ожидающих и еще для того чтобы заглушать голос начальника доносившийся из кабинета так как бесспорно среди посетителей могли находиться и случайные люди\n",
      "Кабинет отличался скромностью присущей Семену Еремеевичу\n",
      "В глубине стоял широкий письменный стол с бронзовыми чернильницами и перед ним два кожаных кресла\n",
      "Справа был стол для заседаний длинный накрытый зеленым сукном и с обеих сторон аккуратно заставленный стульями\n"
     ]
    }
   ],
   "source": [
    "all_train_texts = [' '.join(token.form for token in sent if token.upos not in ['PUNCT']) for sent in full_train]\n",
    "print('\\n'.join(all_train_texts[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab70caf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Можно ADV\n",
      "различать VERB\n",
      "( PUNCT\n",
      "1 NUM\n",
      ") PUNCT\n",
      "большую ADJ\n",
      "науку NOUN\n",
      ", PUNCT\n",
      "( PUNCT\n",
      "2 NUM\n",
      ") PUNCT\n",
      "фундаментальную ADJ\n",
      ", PUNCT\n",
      "( PUNCT\n",
      "3 NUM\n",
      ") PUNCT\n",
      "фундаментальную ADJ\n",
      "социально ADV\n",
      "ориентированную VERB\n",
      "( PUNCT\n",
      "поисковую ADJ\n",
      "? PUNCT\n",
      ") PUNCT\n",
      "None None\n",
      "и CCONJ\n",
      "( PUNCT\n",
      "4 NUM\n",
      ") PUNCT\n",
      "прикладную ADJ\n",
      "науку NOUN\n",
      ". PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in full_test[6587:6588]:\n",
    "    for token in sent:\n",
    "        print(token.form, token.upos)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0a7218f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Можно различать 1 большую науку 2 фундаментальную 3 фундаментальную социально ориентированную поисковую и 4 прикладную науку']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts = [' '.join(token.form for token in sent if token.form is not None and token.upos is not None and token.upos not in ['PUNCT']) for sent in full_test[6587:6588]]\n",
    "test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "073fda9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Первая касается важных проектов государственного значения типа ядерного оружия или космоса']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts = [' '.join(token.form for token in sent if token.upos not in ['PUNCT']) for sent in full_test[6588:6589]]\n",
    "test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62983453",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_texts = [' '.join(token.form for token in sent if token.form is not None and token.upos is not None\\\n",
    "                           and token.upos not in ['PUNCT']) for sent in full_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34e120e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анкета\n",
      "Начальник областного управления связи Семен Еремеевич был человек простой приходил на работу всегда вовремя здоровался с секретаршей за руку и иногда даже писал в стенгазету заметки под псевдонимом Муха\n",
      "В приемной его с утра ожидали посетители кое-кто с важными делами а кое-кто и с такими которые легко можно было решить в нижестоящих инстанциях не затрудняя Семена Еремеевича\n",
      "Однако стиль работы Семена Еремеевича заключался в том чтобы принимать всех желающих и лично вникать в дело\n",
      "Приемная была обставлена просто но по-деловому\n",
      "У двери стоял стол секретарши на столе пишущая машинка с широкой кареткой\n",
      "В углу висел репродуктор и играло радио для развлечения ожидающих и еще для того чтобы заглушать голос начальника доносившийся из кабинета так как бесспорно среди посетителей могли находиться и случайные люди\n",
      "Кабинет отличался скромностью присущей Семену Еремеевичу\n",
      "В глубине стоял широкий письменный стол с бронзовыми чернильницами и перед ним два кожаных кресла\n",
      "Справа был стол для заседаний длинный накрытый зеленым сукном и с обеих сторон аккуратно заставленный стульями\n"
     ]
    }
   ],
   "source": [
    "all_train_labels = [' '.join(token.upos for token in sent if token.form is not None and token.upos is not None\\\n",
    "                             and token.upos not in ['PUNCT']) for sent in full_train]\n",
    "all_test_labels = [' '.join(token.upos for token in sent if token.form is not None and token.upos is not None\\\n",
    "                            and token.upos not in ['PUNCT']) for sent in full_test]\n",
    "print('\\n'.join(all_train_texts[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c54562f",
   "metadata": {},
   "source": [
    "# проверить UnigramTagger, BigramTagger, TrigramTagger и их комбмнации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6dc81d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Это', 'PRON'),\n",
       " ('сочинение', None),\n",
       " ('известно', 'ADJ'),\n",
       " ('во', 'ADP'),\n",
       " ('многих', 'NUM'),\n",
       " ('вариантах', 'NOUN'),\n",
       " ('самые', 'ADJ'),\n",
       " ('ранние', 'ADJ'),\n",
       " ('из', 'ADP'),\n",
       " ('них', 'PRON'),\n",
       " ('почти', 'ADV'),\n",
       " ('на', 'ADP'),\n",
       " ('сто', 'NUM'),\n",
       " ('лет', 'NOUN'),\n",
       " ('старше', 'ADJ'),\n",
       " ('и', 'CCONJ'),\n",
       " ('восходит', 'VERB'),\n",
       " ('к', 'ADP'),\n",
       " ('ещё', 'ADV'),\n",
       " ('более', 'ADV'),\n",
       " ('древним', 'ADJ'),\n",
       " ('рукописям', None),\n",
       " ('XVI', 'NUM'),\n",
       " ('в', 'ADP')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7824266100768464"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unigram_tagger = UnigramTagger(fdata_train)\n",
    "\n",
    "display(unigram_tagger.tag(fdata_sent_test[100]), unigram_tagger.evaluate(fdata_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9d7c1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Это', 'PRON'),\n",
       " ('сочинение', None),\n",
       " ('известно', 'ADJ'),\n",
       " ('во', 'ADP'),\n",
       " ('многих', 'NUM'),\n",
       " ('вариантах', 'NOUN'),\n",
       " ('самые', 'ADJ'),\n",
       " ('ранние', 'ADJ'),\n",
       " ('из', 'ADP'),\n",
       " ('них', 'PRON'),\n",
       " ('почти', 'ADV'),\n",
       " ('на', 'ADP'),\n",
       " ('сто', 'NUM'),\n",
       " ('лет', 'NOUN'),\n",
       " ('старше', 'ADJ'),\n",
       " ('и', 'CCONJ'),\n",
       " ('восходит', 'VERB'),\n",
       " ('к', 'ADP'),\n",
       " ('ещё', 'ADV'),\n",
       " ('более', 'ADV'),\n",
       " ('древним', 'ADJ'),\n",
       " ('рукописям', None),\n",
       " ('XVI', 'NUM'),\n",
       " ('в', 'ADP')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7879650172020193"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bigram_tagger = BigramTagger(fdata_train, backoff=unigram_tagger)\n",
    "\n",
    "display(bigram_tagger.tag(fdata_sent_test[100]), bigram_tagger.evaluate(fdata_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ef01c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Это', 'PRON'),\n",
       " ('сочинение', None),\n",
       " ('известно', 'ADJ'),\n",
       " ('во', 'ADP'),\n",
       " ('многих', 'NUM'),\n",
       " ('вариантах', 'NOUN'),\n",
       " ('самые', 'ADJ'),\n",
       " ('ранние', 'ADJ'),\n",
       " ('из', 'ADP'),\n",
       " ('них', 'PRON'),\n",
       " ('почти', 'ADV'),\n",
       " ('на', 'ADP'),\n",
       " ('сто', 'NUM'),\n",
       " ('лет', 'NOUN'),\n",
       " ('старше', 'ADJ'),\n",
       " ('и', 'CCONJ'),\n",
       " ('восходит', 'VERB'),\n",
       " ('к', 'ADP'),\n",
       " ('ещё', 'ADV'),\n",
       " ('более', 'ADV'),\n",
       " ('древним', 'ADJ'),\n",
       " ('рукописям', None),\n",
       " ('XVI', 'NUM'),\n",
       " ('в', 'ADP')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7869602263592811"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trigram_tagger = TrigramTagger(fdata_train, backoff=bigram_tagger)\n",
    "display(trigram_tagger.tag(fdata_sent_test[100]), trigram_tagger.evaluate(fdata_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f306be26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7865100800617344"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
    "    for cls in tagger_classes:\n",
    "        backoff = cls(train_sents, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "\n",
    "backoff = DefaultTagger('NN') \n",
    "tag = backoff_tagger(fdata_train,  \n",
    "                     [UnigramTagger, BigramTagger, TrigramTagger],  \n",
    "                     backoff = backoff) \n",
    "  \n",
    "tag.evaluate(fdata_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabd3cd8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "После удаления пунктуации скор упал на 3%, но оставлю код на будущее."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f77d36",
   "metadata": {},
   "source": [
    "\n",
    "# 2. написать свой теггер как на занятии, попробовать разные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32ff108d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /home/jb/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/names.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "521ca28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import SequentialBackoffTagger\n",
    "from nltk.corpus import names\n",
    "\n",
    "class NamesTagger(SequentialBackoffTagger):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        SequentialBackoffTagger.__init__(self, *args, **kwargs)\n",
    "        self.name_set = set([n.lower() for n in names.words()])\n",
    "            \n",
    "    def choose_tag(self, tokens, index, history):\n",
    "        word = tokens[index]\n",
    "        if word.lower() in self.name_set:\n",
    "             return 'NNP'\n",
    "        else:\n",
    "             return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3027e69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Katya', 'NNP')]\n",
      "[('Adam', 'NNP')]\n",
      "[('Window', None)]\n"
     ]
    }
   ],
   "source": [
    "nt = NamesTagger()\n",
    "print(nt.tag(['Katya'])) \n",
    "print(nt.tag(['Adam'])) \n",
    "print(nt.tag(['Window'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6b9a75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok = []\n",
    "train_label = []\n",
    "for sent in fdata_train[:]:\n",
    "    sent_list = [tok for tok in sent if tok[0] is not None and tok[1] is not None and tok[1] not in ['PUNCT']]\n",
    "    for tok in sent_list:\n",
    "        train_tok.append(tok[0])\n",
    "        train_label.append(tok[1])\n",
    "        \n",
    "test_tok = []\n",
    "test_label = []\n",
    "for sent in fdata_test[:]:\n",
    "    sent_list = [tok for tok in sent if tok[0] is not None and tok[1] is not None and tok[1] not in ['PUNCT']]\n",
    "    for tok in sent_list:\n",
    "        test_tok.append(tok[0])\n",
    "        test_label.append(tok[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b5187d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'в': 10635, 'и': 10476, 'не': 5245, 'на': 5196, 'что': 3726, 'с': 3334, 'по': 2043, 'к': 1706, 'как': 1659, 'а': 1622, ...})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# надо бы предлоги почистить еще\n",
    "nltk.FreqDist(train_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df873a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'NOUN': 103538, 'VERB': 47731, 'ADJ': 43357, 'ADP': 39344, 'ADV': 22733, 'PRON': 18733, 'CCONJ': 15168, 'PROPN': 14855, 'PART': 13556, 'DET': 10781, ...})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(nltk.FreqDist(train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "686927ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "#import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b28561ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train_enc_labels = le.fit_transform(train_label)\n",
    "\n",
    "test_enc_labels = le.transform(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7889ad9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM',\n",
       "       'PART', 'PRON', 'PROPN', 'SCONJ', 'SYM', 'VERB', 'X'], dtype='<U5')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ae186d",
   "metadata": {},
   "source": [
    "# HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5ae81fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "hvectorizer = HashingVectorizer(ngram_range=(1, 3), analyzer='char', n_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ecc855f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = hvectorizer.fit_transform(train_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2328ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = hvectorizer.transform(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a91bd5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=0, max_iter=100)\n",
    "lr.fit(X_train, train_enc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6154a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5d84ae6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.52      0.55     15103\n",
      "           1       0.77      0.91      0.83     13717\n",
      "           2       0.47      0.25      0.33      7783\n",
      "           3       0.81      0.95      0.88      1390\n",
      "           4       0.86      0.96      0.91      5672\n",
      "           5       0.60      0.62      0.61      4265\n",
      "           6       0.00      0.00      0.00        24\n",
      "           7       0.56      0.71      0.63     36238\n",
      "           8       0.58      0.50      0.54      1734\n",
      "           9       0.76      0.62      0.69      5125\n",
      "          10       0.59      0.54      0.56      7444\n",
      "          11       0.54      0.12      0.19      5473\n",
      "          12       0.76      0.89      0.82      2865\n",
      "          13       1.00      0.69      0.82        62\n",
      "          14       0.63      0.54      0.58     17110\n",
      "          15       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.63    124139\n",
      "   macro avg       0.59      0.55      0.56    124139\n",
      "weighted avg       0.62      0.63      0.61    124139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INTJ and X классы по нулям\n",
    "print(classification_report(test_enc_labels, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "79b2839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hvectorizer = HashingVectorizer(ngram_range=(1, 2), analyzer='word', n_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8f3de3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = hvectorizer.fit_transform(train_tok)\n",
    "X_test = hvectorizer.transform(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5d734806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=0, max_iter=100)\n",
    "lr.fit(X_train, train_enc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9fb679d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e4d812d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     15103\n",
      "           1       0.60      0.37      0.46     13717\n",
      "           2       0.35      0.23      0.28      7783\n",
      "           3       0.32      0.34      0.33      1390\n",
      "           4       0.53      0.12      0.19      5672\n",
      "           5       0.30      0.13      0.18      4265\n",
      "           6       0.00      0.00      0.00        24\n",
      "           7       0.33      0.82      0.47     36238\n",
      "           8       0.00      0.00      0.00      1734\n",
      "           9       0.58      0.57      0.58      5125\n",
      "          10       0.42      0.38      0.40      7444\n",
      "          11       0.16      0.02      0.04      5473\n",
      "          12       0.55      0.70      0.61      2865\n",
      "          13       0.00      0.00      0.00        62\n",
      "          14       0.28      0.02      0.04     17110\n",
      "          15       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.37    124139\n",
      "   macro avg       0.28      0.23      0.22    124139\n",
      "weighted avg       0.33      0.37      0.29    124139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_enc_labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99b77b1",
   "metadata": {},
   "source": [
    "# CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4a4514ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1, 3), analyzer='char', binary=False)\n",
    "# Создаем the Bag-of-Words модель\n",
    "X_train = count_vectorizer.fit_transform(train_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b3372fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('а', 2786),\n",
       " ('н', 7169),\n",
       " ('к', 6165),\n",
       " ('е', 4520),\n",
       " ('т', 9282),\n",
       " ('ан', 3078),\n",
       " ('нк', 7335),\n",
       " ('ке', 6230),\n",
       " ('ет', 4919),\n",
       " ('та', 9300)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(count_vectorizer.vocabulary_.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "558f80fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = count_vectorizer.transform(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "acf6c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=0, max_iter=100)\n",
    "lr.fit(X_train, train_enc_labels)\n",
    "pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "963e66ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.91     15103\n",
      "           1       0.98      1.00      0.99     13717\n",
      "           2       0.90      0.90      0.90      7783\n",
      "           3       0.81      0.97      0.88      1390\n",
      "           4       0.88      0.98      0.93      5672\n",
      "           5       0.89      0.73      0.80      4265\n",
      "           6       0.32      0.29      0.30        24\n",
      "           7       0.92      0.95      0.94     36238\n",
      "           8       0.84      0.89      0.87      1734\n",
      "           9       0.96      0.76      0.85      5125\n",
      "          10       0.83      0.90      0.86      7444\n",
      "          11       0.74      0.60      0.66      5473\n",
      "          12       0.80      0.91      0.85      2865\n",
      "          13       1.00      0.73      0.84        62\n",
      "          14       0.94      0.93      0.94     17110\n",
      "          15       0.53      0.13      0.20       134\n",
      "\n",
      "    accuracy                           0.91    124139\n",
      "   macro avg       0.83      0.79      0.80    124139\n",
      "weighted avg       0.91      0.91      0.91    124139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_enc_labels, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "81b476aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2), analyzer='word', binary=False)\n",
    "# Создаем the Bag-of-Words модель\n",
    "X_train = count_vectorizer.fit_transform(train_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "993f1912",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = count_vectorizer.transform(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e26a48d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=0, max_iter=100)\n",
    "lr.fit(X_train, train_enc_labels)\n",
    "pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "efeab277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.49      0.65     15103\n",
      "           1       0.98      0.48      0.64     13717\n",
      "           2       0.94      0.80      0.86      7783\n",
      "           3       0.82      0.95      0.88      1390\n",
      "           4       0.89      0.20      0.33      5672\n",
      "           5       0.84      0.74      0.78      4265\n",
      "           6       0.00      0.00      0.00        24\n",
      "           7       0.52      0.99      0.68     36238\n",
      "           8       0.87      0.61      0.72      1734\n",
      "           9       0.99      0.73      0.84      5125\n",
      "          10       0.89      0.78      0.83      7444\n",
      "          11       0.91      0.18      0.30      5473\n",
      "          12       0.73      0.98      0.84      2865\n",
      "          13       0.00      0.00      0.00        62\n",
      "          14       0.99      0.59      0.74     17110\n",
      "          15       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.69    124139\n",
      "   macro avg       0.71      0.53      0.57    124139\n",
      "weighted avg       0.81      0.69      0.68    124139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_enc_labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc9433a",
   "metadata": {},
   "source": [
    "# TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a63b1ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='char', binary=False)\n",
    "# Создаем the Bag-of-Words модель\n",
    "X_train = tfidf_vectorizer.fit_transform(train_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "554521fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124139, 11981)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = tfidf_vectorizer.transform(test_tok)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fa18c924",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=0, max_iter=100)\n",
    "lr.fit(X_train, train_enc_labels)\n",
    "pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d1c964a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90     15103\n",
      "           1       0.99      1.00      0.99     13717\n",
      "           2       0.90      0.86      0.88      7783\n",
      "           3       0.84      0.89      0.86      1390\n",
      "           4       0.89      0.97      0.93      5672\n",
      "           5       0.92      0.68      0.78      4265\n",
      "           6       0.00      0.00      0.00        24\n",
      "           7       0.90      0.95      0.92     36238\n",
      "           8       0.83      0.90      0.86      1734\n",
      "           9       0.94      0.78      0.85      5125\n",
      "          10       0.82      0.91      0.86      7444\n",
      "          11       0.77      0.51      0.62      5473\n",
      "          12       0.81      0.91      0.86      2865\n",
      "          13       1.00      0.82      0.90        62\n",
      "          14       0.92      0.93      0.92     17110\n",
      "          15       0.57      0.03      0.06       134\n",
      "\n",
      "    accuracy                           0.90    124139\n",
      "   macro avg       0.81      0.75      0.76    124139\n",
      "weighted avg       0.90      0.90      0.90    124139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_enc_labels, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7f0e1617",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='word', binary=False)\n",
    "# Создаем the Bag-of-Words модель\n",
    "X_train = tfidf_vectorizer.fit_transform(train_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "410ea283",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tfidf_vectorizer.transform(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "885580f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=0, max_iter=100)\n",
    "lr.fit(X_train, train_enc_labels)\n",
    "pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d22e3350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.49      0.65     15103\n",
      "           1       0.99      0.48      0.64     13717\n",
      "           2       0.95      0.79      0.86      7783\n",
      "           3       0.82      0.95      0.88      1390\n",
      "           4       0.98      0.18      0.30      5672\n",
      "           5       0.79      0.77      0.78      4265\n",
      "           6       0.00      0.00      0.00        24\n",
      "           7       0.51      1.00      0.67     36238\n",
      "           8       0.86      0.64      0.73      1734\n",
      "           9       0.94      0.78      0.85      5125\n",
      "          10       0.91      0.73      0.81      7444\n",
      "          11       0.93      0.18      0.30      5473\n",
      "          12       0.76      0.97      0.85      2865\n",
      "          13       0.00      0.00      0.00        62\n",
      "          14       0.98      0.52      0.68     17110\n",
      "          15       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.68    124139\n",
      "   macro avg       0.71      0.53      0.56    124139\n",
      "weighted avg       0.81      0.68      0.67    124139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_enc_labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d4e67e",
   "metadata": {},
   "source": [
    "# ВЫВОД\n",
    "\n",
    "из теггеров лучше всех себя показал BigramTagger, но по сравнению с countbase-подходами они показали меньшую accurasy. Из countbase-подходов лучше всех Countvectoriser. Метрики лучше на символах, хуже на словах. В целом датасет нуждается в дополнительной чистке. Нужно убрать предлоги, возможно цифры и т.д.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4b8360",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
